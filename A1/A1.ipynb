{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPR: 0101001234 -> DD: 01, MM: 01, YY: 00, IIII: 1234\n",
      "CPR: 010203-1234 -> DD: 01, MM: 02, YY: 03, IIII: 1234\n",
      "CPR: 3112995678 -> DD: 31, MM: 12, YY: 99, IIII: 5678\n",
      "CPR: 311299-5678 -> DD: 31, MM: 12, YY: 99, IIII: 5678\n",
      "Invalid CPR: 1234567890\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# a)\n",
    "# six-digit date (DDMMYY) followed by 4 digit identifier (IIII)\n",
    "# show either DDMMYYIIII or DDMMYY-IIII format\n",
    "\n",
    "cpr_pattern = r'\\b((?:0[1-9]|[12][0-9]|3[01]))((?:0[1-9]|1[0-2]))(\\d{2})-?(\\d{4})\\b'\n",
    "\n",
    "def extract_cpr(cpr_str: str):\n",
    "    match = re.fullmatch(cpr_pattern, cpr_str)\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "test_cprs = [\"0101001234\", \"010203-1234\", \"3112995678\", \"311299-5678\", \"1234567890\"]\n",
    "\n",
    "for cpr in test_cprs:\n",
    "    result = extract_cpr(cpr)\n",
    "    if result:\n",
    "        print(f\"CPR: {cpr} -> DD: {result[0]}, MM: {result[1]}, YY: {result[2]}, IIII: {result[3]}\")\n",
    "    else:\n",
    "        print(f\"Invalid CPR: {cpr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YY: 99, IIII: 4000 -> Century: 1900\n",
      "YY: 15, IIII: 4500 -> Century: 2000\n",
      "YY: 50, IIII: 4500 -> Century: 1900\n",
      "YY: 30, IIII: 6000 -> Century: 2000\n",
      "YY: 70, IIII: 6000 -> Century: 1800\n",
      "YY: 20, IIII: 9500 -> Century: 2000\n",
      "YY: 80, IIII: 9500 -> Century: 1900\n"
     ]
    }
   ],
   "source": [
    "# b)\n",
    "# Function to return relevant centuary\n",
    "def centuarty(yy: int, iiii: int) -> int:\n",
    "    if 1 <= iiii <= 3999:\n",
    "        return 1900\n",
    "    elif 4000 <= iiii <= 4999:\n",
    "        return 2000 if 0 <= yy <= 36 else 1900\n",
    "    elif 5000 <= iiii <= 8999:\n",
    "        return 2000 if 0 <= yy <= 57 else 1800\n",
    "    elif 9000 <= iiii <= 9999:\n",
    "        return 2000 if 0 <= yy <= 36 else 1900\n",
    "    else:\n",
    "        raise ValueError(\"Invalid CPR (IIII)\")\n",
    "    \n",
    "# Test cases\n",
    "test_cases = [\n",
    "    (99, 4000),  # 1900\n",
    "    (15, 4500),  # 2000\n",
    "    (50, 4500),  # 1900\n",
    "    (30, 6000),  # 2000\n",
    "    (70, 6000),  # 1800\n",
    "    (20, 9500),  # 2000\n",
    "    (80, 9500),  # 1900\n",
    "]\n",
    "\n",
    "for yy, iiii in test_cases:\n",
    "    print(f\"YY: {yy}, IIII: {iiii} -> Century: {centuarty(yy, iiii)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1-2)\n",
    "# Read csv file\n",
    "df = pd.read_csv(\"news_sample.csv\")\n",
    "\n",
    "# display head\n",
    "#print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        250 non-null    int64  \n",
      " 1   id                250 non-null    int64  \n",
      " 2   domain            250 non-null    object \n",
      " 3   type              238 non-null    object \n",
      " 4   url               250 non-null    object \n",
      " 5   content           250 non-null    object \n",
      " 6   scraped_at        250 non-null    object \n",
      " 7   inserted_at       250 non-null    object \n",
      " 8   updated_at        250 non-null    object \n",
      " 9   title             250 non-null    object \n",
      " 10  authors           170 non-null    object \n",
      " 11  keywords          0 non-null      float64\n",
      " 12  meta_keywords     250 non-null    object \n",
      " 13  meta_description  54 non-null     object \n",
      " 14  tags              27 non-null     object \n",
      " 15  summary           0 non-null      float64\n",
      "dtypes: float64(2), int64(2), object(12)\n",
      "memory usage: 31.4+ KB\n",
      "None \n",
      "\n",
      "0    Sometimes the power of Christmas will make you...\n",
      "1    AWAKENING OF 12 STRANDS of DNA – “Reconnecting...\n",
      "2    Never Hike Alone: A Friday the 13th Fan Film U...\n",
      "3    When a rare shark was caught, scientists were ...\n",
      "4    Donald Trump has the unnerving ability to abil...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 3)\n",
    "# Inspect the Data\n",
    "print(\"Data Info:\")\n",
    "print(df.info(), \"\\n\")\n",
    "\n",
    "# print head of content column \n",
    "print(df.head()['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above when we print out df.info() out, I get provided a summary of my Data, which is useful for serveral purposes:\n",
    "\n",
    "# Understanding structure of the data\n",
    "* Number of rows and columns: here we see 250 rows and 16 (0 indexed) columns.\n",
    "* Column names and data types: Helps me understand what kind of that every column has (float64, int64, object).\n",
    "* Memory usage, The news_sample.csv takes 31.4 KB of memory.\n",
    "\n",
    "# Identifying missing data\n",
    "* the type, authors, meta_description, and tags columns have missing values.\n",
    "* the keywords and summary columns are completely empty, as seen with 0 non-null.\n",
    "\n",
    "# Unnecessary columns\n",
    "* unnamed: 0 looks unnecessary\n",
    "* keywords and summary are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    sometimes the power of christmas will make you...\n",
      "1    awakening of <NUM> strands of dna – “reconnect...\n",
      "2    never hike alone: a friday the 13th fan film u...\n",
      "3    when a rare shark was caught, scientists were ...\n",
      "4    donald trump has the unnerving ability to abil...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 4. clean data using clean_tex() that uses regular expressions\n",
    "\n",
    "def clean_text(text):\n",
    "    # make sure input is a str\n",
    "    if not isinstance(text, str): \n",
    "        return text\n",
    "    # all words must be lowercased\n",
    "    text = text.lower() \n",
    "    # it should not contain multiple white spaces, tabs, or new lines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Replace common date formats\n",
    "    text = re.sub(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b', '<DATE>', text)\n",
    "    # Month DD, YYYY\n",
    "    text = re.sub(r'\\b(?:january|february|march|april|may|june|july|august|september|october|november|december) \\d{1,2}, \\d{4}\\b', '<DATE>', text, flags=re.IGNORECASE)\n",
    "    # YYYY-MM-DD with optional timestamp\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}(?: \\d{2}:\\d{2}:\\d{2}(?:\\.\\d{1,6})?)?\\b', '<DATE>', text)\n",
    "    \n",
    "    # Replace standalone numbers\n",
    "    text = re.sub(r'\\b\\d+\\b', '<NUM>', text)\n",
    "    # Replace emails\n",
    "    text = re.sub(r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b', '<EMAIL>', text)\n",
    "    # Replace URLs (both for http and https (s? is optional))\n",
    "    text = re.sub(r'\\b(?:https?|ftp):\\/\\/[^\\s/$.?#].[^\\s]*\\b', '<URL>', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to text-based columns\n",
    "df['content'] = df['content'].apply(clean_text)\n",
    "\n",
    "# print head of content column \n",
    "print(df.head()['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here above we can see the changes made to the content column, or at least the .head() of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5)\n",
    "import unicodedata\n",
    "from cleantext import clean\n",
    "\n",
    "def clean_text(text):\n",
    "    # Ensure input is string\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    \n",
    "    # Normalize unicode chars to closest ASCII\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "    \n",
    "    text = clean(text,\n",
    "                 lower=True,       # Convert to lowercase\n",
    "                 no_urls=True,     # Replace URLs with <URL>\n",
    "                 no_emails=True,   # Replace emails with <EMAIL>\n",
    "                 no_numbers=True,  # Replace numbers with <NUM>\n",
    "                 no_punct=False,   # Keep punctuation\n",
    "                 replace_with_url='<URL>',\n",
    "                 replace_with_email='<EMAIL>',\n",
    "                 replace_with_number='<NUM>',\n",
    "                 replace_with_punct=\"\")\n",
    "    \n",
    "    text = re.sub(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b', '<DATE>', text)  # Replace date format\n",
    "    text = re.sub(r'\\b(?:january|february|march|april|may|june|july|august|september|october|november|december) \\d{1,2}, \\d{4}\\b', '<DATE>', text, flags=re.IGNORECASE)  # Month DD, YYYY\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '<DATE>', text)  # YYYY-MM-DD\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning function\n",
    "df['content'] = df['content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Mariu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleantext import clean\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# 1: Read the CSV file and save the original\n",
    "df = pd.read_csv(\"news_sample.csv\")\n",
    "\n",
    "# Save the original raw text before cleaning\n",
    "df['raw_content'] = df['content']\n",
    "\n",
    "def clean_text_lib(text):\n",
    "   return clean(text,\n",
    "               lower=True,\n",
    "               no_line_breaks=True,\n",
    "               no_urls=True,\n",
    "               no_emails=True,\n",
    "               no_numbers=True,\n",
    "               no_punct=True)\n",
    "    \n",
    "# 2: Clean the data \n",
    "df['clean_content'] = df['content'].apply(lambda x: clean_text_lib(x) if isinstance(x, str) else \"\")\n",
    "\n",
    "# 3: get all raw data from 'content'\n",
    "raw_text = \" \".join(df['content'].dropna().tolist())\n",
    "raw_tokens = word_tokenize(raw_text)\n",
    "unique_raw_words = set(raw_tokens)\n",
    "print(\"unique words in raw text:\", len(unique_raw_words))\n",
    "\n",
    "# Get all cleaned text from the new \"clean_content\" \n",
    "clean_text_all = \" \".join(df['clean_content'].dropna().tolist())\n",
    "clean_tokens = word_tokenize(clean_text_all)\n",
    "unique_clean_words = set(clean_tokens)\n",
    "print(\"unique words in cleaned text:\", len(unique_clean_words))\n",
    "\n",
    "# number of each word in cleaned text\n",
    "word_freq = Counter(clean_tokens)\n",
    "most_common_50 = word_freq.most_common(50)\n",
    "\n",
    "# Extract words and frequencies\n",
    "words, frequencies = zip(*most_common_50)\n",
    "\n",
    "# Plot the 50 most frequent words\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(words, frequencies)\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"50 Most frequent words in cleaned text\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
