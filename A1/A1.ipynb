{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid CPR: 0102031234 -> DD: 01, MM: 02, YY: 03, IIII: 1234\n",
      "Valid CPR: 010203-1234 -> DD: 01, MM: 02, YY: 03, IIII: 1234\n",
      "Valid CPR: 3112995678 -> DD: 31, MM: 12, YY: 99, IIII: 5678\n",
      "Valid CPR: 311299-5678 -> DD: 31, MM: 12, YY: 99, IIII: 5678\n",
      "Invalid CPR: 1234567890\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# a)\n",
    "# six-digit date (DDMMYY) followed by 4 digit identifier (IIII)\n",
    "# show either DDMMYYIIII or DDMMYY-IIII format\n",
    "\n",
    "cpr_pattern = r'\\b((?:0[1-9]|[12][0-9]|3[01]))((?:0[1-9]|1[0-2]))(\\d{2})-?(\\d{4})\\b'\n",
    "\n",
    "def extract_cpr(cpr_str: str):\n",
    "    match = re.fullmatch(cpr_pattern, cpr_str)\n",
    "    if match:\n",
    "        return match.groups()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "test_cprs = [\"0102031234\", \"010203-1234\", \"3112995678\", \"311299-5678\", \"1234567890\"]\n",
    "\n",
    "for cpr in test_cprs:\n",
    "    result = extract_cpr(cpr)\n",
    "    if result:\n",
    "        print(f\"Valid CPR: {cpr} -> DD: {result[0]}, MM: {result[1]}, YY: {result[2]}, IIII: {result[3]}\")\n",
    "    else:\n",
    "        print(f\"Invalid CPR: {cpr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YY: 99, IIII: 1234 -> Century: 1900\n",
      "YY: 15, IIII: 4500 -> Century: 2000\n",
      "YY: 50, IIII: 4500 -> Century: 1900\n",
      "YY: 30, IIII: 6000 -> Century: 2000\n",
      "YY: 70, IIII: 6000 -> Century: 1800\n",
      "YY: 20, IIII: 9500 -> Century: 2000\n",
      "YY: 80, IIII: 9500 -> Century: 1900\n"
     ]
    }
   ],
   "source": [
    "# b)\n",
    "# Function to return relevant centuary\n",
    "def centuarty(yy: int, iiii: int) -> int:\n",
    "    if 1 <= iiii <= 3999:\n",
    "        return 1900\n",
    "    elif 4000 <= iiii <= 4999:\n",
    "        return 2000 if 0 <= yy <= 36 else 1900\n",
    "    elif 5000 <= iiii <= 8999:\n",
    "        return 2000 if 0 <= yy <= 57 else 1800\n",
    "    elif 9000 <= iiii <= 9999:\n",
    "        return 2000 if 0 <= yy <= 36 else 1900\n",
    "    else:\n",
    "        raise ValueError(\"Invalid CPR identifier (IIII)\")\n",
    "    \n",
    "# Test cases\n",
    "test_cases = [\n",
    "    (99, 1234),  # 1900\n",
    "    (15, 4500),  # 2000\n",
    "    (50, 4500),  # 1900\n",
    "    (30, 6000),  # 2000\n",
    "    (70, 6000),  # 1800\n",
    "    (20, 9500),  # 2000\n",
    "    (80, 9500),  # 1900\n",
    "]\n",
    "\n",
    "for yy, iiii in test_cases:\n",
    "    print(f\"YY: {yy}, IIII: {iiii} -> Century: {centuarty(yy, iiii)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1-2)\n",
    "# Read csv file\n",
    "df = pd.read_csv(\"news_sample.csv\")\n",
    "\n",
    "# display head\n",
    "#print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 250 entries, 0 to 249\n",
      "Data columns (total 16 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   Unnamed: 0        250 non-null    int64  \n",
      " 1   id                250 non-null    int64  \n",
      " 2   domain            250 non-null    object \n",
      " 3   type              238 non-null    object \n",
      " 4   url               250 non-null    object \n",
      " 5   content           250 non-null    object \n",
      " 6   scraped_at        250 non-null    object \n",
      " 7   inserted_at       250 non-null    object \n",
      " 8   updated_at        250 non-null    object \n",
      " 9   title             250 non-null    object \n",
      " 10  authors           170 non-null    object \n",
      " 11  keywords          0 non-null      float64\n",
      " 12  meta_keywords     250 non-null    object \n",
      " 13  meta_description  54 non-null     object \n",
      " 14  tags              27 non-null     object \n",
      " 15  summary           0 non-null      float64\n",
      "dtypes: float64(2), int64(2), object(12)\n",
      "memory usage: 31.4+ KB\n",
      "None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3)\n",
    "# 3. Inspect the DataFrame\n",
    "print(\"DataFrame Info:\")\n",
    "print(df.info(), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above when we print out df.info() out, I get provided a summary of my Data, which is useful for serveral purposes:\n",
    "\n",
    "# Understanding structure of the data\n",
    "* Number of rows and columns: here we see 250 rows and 16 (0 indexed) columns.\n",
    "* Column names and data types: Helps me understand what kind of that every column has (float64, int64, object).\n",
    "* Memory usage, The news_sample.csv takes 31.4 KB of memory.\n",
    "\n",
    "# Identifying missing data\n",
    "* the type, authors, meta_description, and tags columns have missing values.\n",
    "* the keywords and summary columns are completely empty, as seen with 0 non-null.\n",
    "\n",
    "# Unnecessary columns\n",
    "* unnamed: 0 looks unnecessary\n",
    "* keywords and summary are empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. clean data using clean_tex() that uses regular expressions\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Ensure input is string\n",
    "        return text\n",
    "    # all words must be lowercased\n",
    "    text = text.lower() \n",
    "    # it should not contain multiple white spaces, tabs, or new lines\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Replace common date formats\n",
    "    text = re.sub(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b', '<DATE>', text)\n",
    "    # Month DD, YYYY\n",
    "    text = re.sub(r'\\b(?:january|february|march|april|may|june|july|august|september|october|november|december) \\d{1,2}, \\d{4}\\b', '<DATE>', text, flags=re.IGNORECASE)\n",
    "    # YYYY-MM-DD with optional timestamp\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}(?: \\d{2}:\\d{2}:\\d{2}(?:\\.\\d{1,6})?)?\\b', '<DATE>', text)\n",
    "    \n",
    "    # Replace standalone numbers\n",
    "    text = re.sub(r'\\b\\d+\\b', '<NUM>', text)\n",
    "    # Replace emails\n",
    "    text = re.sub(r'\\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b', '<EMAIL>', text)\n",
    "    # Replace URLs\n",
    "    text = re.sub(r'\\b(?:https?|ftp):\\/\\/[^\\s/$.?#].[^\\s]*\\b', '<URL>', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to text-based columns\n",
    "df['content'] = df['content'].apply(clean_text)\n",
    "df['title'] = df['title'].apply(clean_text)\n",
    "df['meta_description'] = df['meta_description'].apply(clean_text)\n",
    "df['tags'] = df['tags'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5)\n",
    "import unicodedata\n",
    "from cleantext import clean\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):  # Ensure input is string\n",
    "        return text\n",
    "    \n",
    "    # Normalize Unicode characters to closest ASCII equivalent\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "    \n",
    "    text = clean(text,\n",
    "                 lower=True,  # Convert to lowercase\n",
    "                 no_urls=True,  # Replace URLs with <URL>\n",
    "                 no_emails=True,  # Replace emails with <EMAIL>\n",
    "                 no_numbers=True,  # Replace numbers with <NUM>\n",
    "                 no_punct=False,  # Keep punctuation\n",
    "                 replace_with_url='<URL>',\n",
    "                 replace_with_email='<EMAIL>',\n",
    "                 replace_with_number='<NUM>',\n",
    "                 replace_with_punct=\"\")\n",
    "    \n",
    "    text = re.sub(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b', '<DATE>', text)  # Replace common date formats\n",
    "    text = re.sub(r'\\b(?:january|february|march|april|may|june|july|august|september|october|november|december) \\d{1,2}, \\d{4}\\b', '<DATE>', text, flags=re.IGNORECASE)  # Month DD, YYYY\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '<DATE>', text)  # YYYY-MM-DD\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply cleaning function to text-based columns\n",
    "df['content'] = df['content'].apply(clean_text)\n",
    "df['title'] = df['title'].apply(clean_text)\n",
    "df['meta_description'] = df['meta_description'].apply(clean_text)\n",
    "df['tags'] = df['tags'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcleantext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clean\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      8\u001b[0m nltk\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 1: Read the CSV file and save the original\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mariu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\__init__.py:180\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download, download_shell\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtkinter\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mariu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\downloader.py:2475\u001b[0m\n\u001b[0;32m   2465\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   2468\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[0;32m   2469\u001b[0m \u001b[38;5;66;03m# Main:\u001b[39;00m\n\u001b[0;32m   2470\u001b[0m \u001b[38;5;66;03m######################################################################\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2473\u001b[0m \n\u001b[0;32m   2474\u001b[0m \u001b[38;5;66;03m# Aliases\u001b[39;00m\n\u001b[1;32m-> 2475\u001b[0m _downloader \u001b[38;5;241m=\u001b[39m \u001b[43mDownloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2476\u001b[0m download \u001b[38;5;241m=\u001b[39m _downloader\u001b[38;5;241m.\u001b[39mdownload\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_shell\u001b[39m():\n",
      "File \u001b[1;32mc:\\Users\\Mariu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\downloader.py:515\u001b[0m, in \u001b[0;36mDownloader.__init__\u001b[1;34m(self, server_index_url, download_dir)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;66;03m# decide where we're going to save things to.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_download_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mariu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\downloader.py:1068\u001b[0m, in \u001b[0;36mDownloader.default_download_dir\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m \u001b[38;5;66;03m# Check if we have sufficient permissions to install in a\u001b[39;00m\n\u001b[0;32m   1067\u001b[0m \u001b[38;5;66;03m# variety of system-wide locations.\u001b[39;00m\n\u001b[1;32m-> 1068\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m nltkdir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mpath:\n\u001b[0;32m   1069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(nltkdir) \u001b[38;5;129;01mand\u001b[39;00m nltk\u001b[38;5;241m.\u001b[39minternals\u001b[38;5;241m.\u001b[39mis_writable(nltkdir):\n\u001b[0;32m   1070\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m nltkdir\n",
      "\u001b[1;31mAttributeError\u001b[0m: partially initialized module 'nltk' has no attribute 'data' (most likely due to a circular import)"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleantext import clean\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 1: Read the CSV file and save the original\n",
    "df = pd.read_csv(\"news_sample.csv\")\n",
    "\n",
    "# Save the original raw text before cleaning\n",
    "df['raw_content'] = df['content']\n",
    "\n",
    "def clean_text_lib(text):\n",
    "   return clean(text, lower=True, no_line_breaks=True, no_urls=True, no_emails=True, no_numbers=True)\n",
    "\n",
    "# 2: Clean the data \n",
    "df['clean_content'] = df['content'].apply(lambda x: clean_text_lib(x) if isinstance(x, str) else \"\")\n",
    "\n",
    "# 3: get all raw data from 'content'\n",
    "raw_text = \" \".join(df['content'].dropna().tolist())\n",
    "raw_tokens = word_tokenize(raw_text)\n",
    "unique_raw_words = set(raw_tokens)\n",
    "print(\"unique words in raw text:\", len(unique_raw_words))\n",
    "\n",
    "# Get all cleaned text from the new \"clean_content\" column\n",
    "clean_text_all = \" \".join(df['clean_content'].dropna().tolist())\n",
    "clean_tokens = word_tokenize(clean_text_all)\n",
    "unique_clean_words = set(clean_tokens)\n",
    "print(\"unique words in cleaned text:\", len(unique_clean_words))\n",
    "\n",
    "# number of each word in cleaned text\n",
    "word_freq = Counter(clean_tokens)\n",
    "most_common_50 = word_freq.most_common(50)\n",
    "\n",
    "# Extract words and frequencies\n",
    "words, frequencies = zip(*most_common_50)\n",
    "\n",
    "# Plot the 50 most frequent words\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(words, frequencies)\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"50 Most frequent words in cleaned text\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
