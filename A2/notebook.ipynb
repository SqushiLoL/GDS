{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Analyze the Fake News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Mariu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words in raw text: 20948\n",
      "unique words in cleaned text: 16608\n",
      "type\n",
      "fake        186\n",
      "reliable     21\n",
      "Name: count, dtype: int64\n",
      "type\n",
      "fake        0.898551\n",
      "reliable    0.101449\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1: Import Dataset\n",
    "\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleantext import clean\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# 1: Read the CSV file and save the original\n",
    "df = pd.read_csv(\"news_sample.csv\")\n",
    "\n",
    "# Save the original raw text before cleaning\n",
    "df['raw_content'] = df['content']\n",
    "\n",
    "def clean_text_lib(text):\n",
    "   return clean(text,\n",
    "               lower=True,\n",
    "               no_line_breaks=True,\n",
    "               no_urls=True,\n",
    "               no_emails=True,\n",
    "               no_numbers=True,\n",
    "               no_punct=True)\n",
    "    \n",
    "# 2: Clean the data \n",
    "df['clean_content'] = df['content'].apply(lambda x: clean_text_lib(x) if isinstance(x, str) else \"\")\n",
    "\n",
    "# 3: get all raw data from 'content'\n",
    "raw_text = \" \".join(df['content'].dropna().tolist())\n",
    "raw_tokens = word_tokenize(raw_text)\n",
    "unique_raw_words = set(raw_tokens)\n",
    "print(\"unique words in raw text:\", len(unique_raw_words))\n",
    "\n",
    "# Get all cleaned text from the new \"clean_content\" \n",
    "clean_text_all = \" \".join(df['clean_content'].dropna().tolist())\n",
    "clean_tokens = word_tokenize(clean_text_all)\n",
    "unique_clean_words = set(clean_tokens)\n",
    "print(\"unique words in cleaned text:\", len(unique_clean_words))\n",
    "\n",
    "# number of each word in cleaned text\n",
    "word_freq = Counter(clean_tokens)\n",
    "most_common_50 = word_freq.most_common(50)\n",
    "\n",
    "# Extract words and frequencies\n",
    "words, frequencies = zip(*most_common_50)\n",
    "\n",
    "# Plot the 50 most frequent words\n",
    "#plt.figure(figsize=(15, 5))\n",
    "#plt.bar(words, frequencies)\n",
    "#plt.xlabel(\"Words\")\n",
    "#plt.ylabel(\"Frequency\")\n",
    "#plt.title(\"50 Most frequent words in cleaned text\")\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2: Dataset Analysis\n",
    "\n",
    "# A: Determine which article types should be omitted, if any.\n",
    "\n",
    "# leave out satire (just humor)\n",
    "df = df[df['type'] != 'satire']\n",
    "# Has no data\n",
    "df = df[df['type'] != 'state']\n",
    "# misleading science\n",
    "df = df[df['type'] != 'junksci']\n",
    "# not reliable, just hate\n",
    "df = df[df['type'] != 'hate']\n",
    "# exaggerated\n",
    "df = df[df['type'] != 'clickbait']\n",
    "# specefic viewpoint\n",
    "df = df[df['type'] != 'political']\n",
    "\n",
    "\n",
    "\n",
    "# B: Group the remaining types into 'fake' and 'reliable'. Argue for your choice.\n",
    "\n",
    "# fake\n",
    "df['type'] = df['type'].replace(['conspiracy', 'fake'], 'fake')\n",
    "\n",
    "\n",
    "#reliable\n",
    "df['type'] = df['type'].replace(['unreliable', 'bias', 'unreliable', 'unknown'], 'reliable')\n",
    "print(df['type'].value_counts())\n",
    "\n",
    "# C: Examine the percentage distribution of 'reliable' vs. 'fake' articles. Is the dataset balanced? Discuss the importance of a balanced distribution.\n",
    "\n",
    "# reliable vs fake - percentage\n",
    "print(df['type'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gathering Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookies rejected.\n",
      "\n",
      "--- Scraping region: world/europe ---\n",
      "Extracted 27 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 2...\n",
      "Extracted 6 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 3...\n",
      "Extracted 5 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 4...\n",
      "Extracted 8 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 5...\n",
      "Extracted 0 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 6...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 7...\n",
      "Extracted 8 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 8...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 9...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 10...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 11...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 12...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 13...\n",
      "Extracted 1 new articles from this page.\n",
      "No more pages (button disabled) at page 13.\n",
      "\n",
      "--- Scraping region: world/us_and_canada ---\n",
      "Extracted 26 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 2...\n",
      "Extracted 6 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 3...\n",
      "Extracted 7 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 4...\n",
      "Extracted 8 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 5...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 6...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 7...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 8...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 9...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 10...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 11...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 12...\n",
      "Extracted 0 new articles from this page.\n",
      "No more pages (button disabled) at page 12.\n",
      "\n",
      "--- Scraping region: uk ---\n",
      "Extracted 32 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 2...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 3...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 4...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 5...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 6...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 7...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 8...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 9...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 10...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 11...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 12...\n",
      "Extracted 1 new articles from this page.\n",
      "No more pages (button disabled) at page 12.\n",
      "\n",
      "--- Scraping region: world/australia ---\n",
      "Extracted 18 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 2...\n",
      "Extracted 8 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 3...\n",
      "Extracted 7 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 4...\n",
      "Extracted 8 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 5...\n",
      "Extracted 8 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 6...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 7...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 8...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 9...\n",
      "Extracted 8 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 10...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 11...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 12...\n",
      "Extracted 1 new articles from this page.\n",
      "No more pages (button disabled) at page 12.\n",
      "\n",
      "--- Scraping region: world/asia ---\n",
      "Extracted 18 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 2...\n",
      "Extracted 8 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 3...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 4...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 5...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 6...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 7...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 8...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 9...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 10...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 11...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 12...\n",
      "Extracted 1 new articles from this page.\n",
      "No more pages (button disabled) at page 12.\n",
      "\n",
      "--- Scraping region: world/africa ---\n",
      "Extracted 24 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 2...\n",
      "Extracted 7 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 3...\n",
      "Extracted 6 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 4...\n",
      "Extracted 8 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 5...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 6...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 7...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 8...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 9...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 10...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 11...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 12...\n",
      "Extracted 1 new articles from this page.\n",
      "No more pages (button disabled) at page 12.\n",
      "\n",
      "--- Scraping region: world/latin_america ---\n",
      "Extracted 15 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 2...\n",
      "Extracted 6 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 3...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 4...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 5...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 6...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 7...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 8...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 9...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 10...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 11...\n",
      "Extracted 9 new articles from this page.\n",
      "No more pages (button disabled) at page 11.\n",
      "\n",
      "--- Scraping region: world/middle_east ---\n",
      "Extracted 23 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 2...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 3...\n",
      "Extracted 6 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 4...\n",
      "Extracted 7 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 5...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 6...\n",
      "Extracted 7 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 7...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 8...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 9...\n",
      "Extracted 0 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 10...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 11...\n",
      "Extracted 9 new articles from this page.\n",
      "Clicked 'Next Page' -> now on page 12...\n",
      "Extracted 9 new articles from this page.\n",
      "No more pages (button disabled) at page 12.\n",
      "\n",
      "Done! Extracted 870 unique articles in total.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 1) Initialize Selenium WebDriver (Firefox)\n",
    "# ----------------------------------------------------------------\n",
    "browser = webdriver.Firefox()\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "# Track visited links to avoid duplicates\n",
    "extracted_links = set()\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2) Reject cookies only once\n",
    "# ----------------------------------------------------------------\n",
    "def reject_cookies_once(driver):\n",
    "    try:\n",
    "        iframe = wait.until(\n",
    "            EC.frame_to_be_available_and_switch_to_it(\n",
    "                (By.CSS_SELECTOR, \"iframe[id^='sp_message_iframe']\")\n",
    "            )\n",
    "        )\n",
    "        reject_button = wait.until(\n",
    "            EC.element_to_be_clickable(\n",
    "                (By.XPATH, \"//button[contains(text(), 'I do not agree')]\")\n",
    "            )\n",
    "        )\n",
    "        reject_button.click()\n",
    "        print(\"Cookies rejected.\")\n",
    "    except TimeoutException:\n",
    "        print(\"No cookie popup detected (maybe already dismissed).\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error handling cookies: {e}\")\n",
    "    finally:\n",
    "        driver.switch_to.default_content()\n",
    "        time.sleep(2)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 3) Extract articles from the current page\n",
    "# ----------------------------------------------------------------\n",
    "def extract_articles(driver):\n",
    "    articles = []\n",
    "    \n",
    "    # Wait for at least one article\n",
    "    try:\n",
    "        wait.until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article a[data-testid='internal-link']\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(\"No articles found on this page.\")\n",
    "        return articles\n",
    "    \n",
    "    links = driver.find_elements(By.CSS_SELECTOR, \"article a[data-testid='internal-link']\")\n",
    "    \n",
    "    for link_el in links:\n",
    "        try:\n",
    "            link = link_el.get_attribute(\"href\")\n",
    "            title = link_el.text.strip() or \"No title\"\n",
    "            summary = \"No summary\"  # Adjust if summaries are available\n",
    "            \n",
    "            if link not in extracted_links:\n",
    "                extracted_links.add(link)\n",
    "                articles.append({\n",
    "                    \"title\": title,\n",
    "                    \"summary\": summary,\n",
    "                    \"link\": link\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting an article: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Extracted {len(articles)} new articles from this page.\")\n",
    "    return articles\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 4) Scrape all pages by clicking the “Next Page” button\n",
    "# ----------------------------------------------------------------\n",
    "def scrape_region(driver, url):\n",
    "    all_articles = []\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    page_number = 1\n",
    "    max_pages = 50  # Limit max pages per region to prevent infinite loops\n",
    "    \n",
    "    while page_number <= max_pages:\n",
    "        page_articles = extract_articles(driver)\n",
    "        all_articles.extend(page_articles)\n",
    "        \n",
    "        try:\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, \"button[data-testid='pagination-next-button']\")\n",
    "            \n",
    "            if next_button.get_attribute(\"disabled\"):\n",
    "                print(f\"No more pages (button disabled) at page {page_number}.\")\n",
    "                break\n",
    "\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_button)\n",
    "            time.sleep(1)\n",
    "\n",
    "            try:\n",
    "                next_button.click()\n",
    "            except ElementClickInterceptedException:\n",
    "                ActionChains(driver).move_to_element(next_button).click().perform()\n",
    "\n",
    "            page_number += 1\n",
    "            print(f\"Clicked 'Next Page' -> now on page {page_number}...\")\n",
    "            time.sleep(3)\n",
    "        \n",
    "        except NoSuchElementException:\n",
    "            print(f\"No 'Next Page' button found after page {page_number}.\")\n",
    "            break\n",
    "        except ElementNotInteractableException:\n",
    "            print(f\"'Next Page' button present but not interactable on page {page_number}.\")\n",
    "            break\n",
    "    \n",
    "    return all_articles\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# MAIN SCRIPT\n",
    "# ----------------------------------------------------------------\n",
    "try:\n",
    "    browser.get(\"https://www.bbc.com/news/world/europe\")\n",
    "    time.sleep(3)\n",
    "    reject_cookies_once(browser)\n",
    "\n",
    "    regions = [\n",
    "        \"world/europe\", \"world/us_and_canada\", \"uk\", \"world/australia\", \"world/asia\",\n",
    "        \"world/africa\", \"world/latin_america\", \"world/middle_east\"\n",
    "    ]\n",
    "    \n",
    "    all_articles = []\n",
    "    \n",
    "    for region in regions:\n",
    "        print(f\"\\n--- Scraping region: {region} ---\")\n",
    "        url = f\"https://www.bbc.com/news/{region.lstrip('/')}\"\n",
    "        region_articles = scrape_region(browser, url)\n",
    "        all_articles.extend(region_articles)\n",
    "        time.sleep(2)\n",
    "        \n",
    "finally:\n",
    "    browser.quit()\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"bbc_articles.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"summary\", \"link\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_articles)\n",
    "\n",
    "print(f\"\\nDone! Extracted {len(all_articles)} unique articles in total.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Scraping Article Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1: Article Inspection\n",
    "\n",
    "# 2: Text Scraping Function\n",
    "\n",
    "# 3: Scrape All Articles\n",
    "\n",
    "# 4: Data Storage\n",
    "\n",
    "# 5: Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Preservation\n",
    "Keep the data that you have scraped so you can use it for your Group Project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
