{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Analyze the Fake News Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Mariu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words in raw text: 20948\n",
      "unique words in cleaned text: 16608\n",
      "type\n",
      "fake        186\n",
      "reliable     21\n",
      "Name: count, dtype: int64\n",
      "type\n",
      "fake        0.898551\n",
      "reliable    0.101449\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 1: Import Dataset\n",
    "\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from cleantext import clean\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# 1: Read the CSV file and save the original\n",
    "df = pd.read_csv(\"news_sample.csv\")\n",
    "\n",
    "# Save the original raw text before cleaning\n",
    "df['raw_content'] = df['content']\n",
    "\n",
    "def clean_text_lib(text):\n",
    "   return clean(text,\n",
    "               lower=True,\n",
    "               no_line_breaks=True,\n",
    "               no_urls=True,\n",
    "               no_emails=True,\n",
    "               no_numbers=True,\n",
    "               no_punct=True)\n",
    "    \n",
    "# 2: Clean the data \n",
    "df['clean_content'] = df['content'].apply(lambda x: clean_text_lib(x) if isinstance(x, str) else \"\")\n",
    "\n",
    "# 3: get all raw data from 'content'\n",
    "raw_text = \" \".join(df['content'].dropna().tolist())\n",
    "raw_tokens = word_tokenize(raw_text)\n",
    "unique_raw_words = set(raw_tokens)\n",
    "print(\"unique words in raw text:\", len(unique_raw_words))\n",
    "\n",
    "# Get all cleaned text from the new \"clean_content\" \n",
    "clean_text_all = \" \".join(df['clean_content'].dropna().tolist())\n",
    "clean_tokens = word_tokenize(clean_text_all)\n",
    "unique_clean_words = set(clean_tokens)\n",
    "print(\"unique words in cleaned text:\", len(unique_clean_words))\n",
    "\n",
    "# number of each word in cleaned text\n",
    "word_freq = Counter(clean_tokens)\n",
    "most_common_50 = word_freq.most_common(50)\n",
    "\n",
    "# Extract words and frequencies\n",
    "words, frequencies = zip(*most_common_50)\n",
    "\n",
    "# Plot the 50 most frequent words\n",
    "#plt.figure(figsize=(15, 5))\n",
    "#plt.bar(words, frequencies)\n",
    "#plt.xlabel(\"Words\")\n",
    "#plt.ylabel(\"Frequency\")\n",
    "#plt.title(\"50 Most frequent words in cleaned text\")\n",
    "#plt.xticks(rotation=90)\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2: Dataset Analysis\n",
    "\n",
    "# A: Determine which article types should be omitted, if any.\n",
    "\n",
    "# leave out satire (just humor)\n",
    "df = df[df['type'] != 'satire']\n",
    "# Has no data\n",
    "df = df[df['type'] != 'state']\n",
    "# misleading science\n",
    "df = df[df['type'] != 'junksci']\n",
    "# not reliable, just hate\n",
    "df = df[df['type'] != 'hate']\n",
    "# exaggerated\n",
    "df = df[df['type'] != 'clickbait']\n",
    "# specefic viewpoint\n",
    "df = df[df['type'] != 'political']\n",
    "\n",
    "\n",
    "\n",
    "# B: Group the remaining types into 'fake' and 'reliable'. Argue for your choice.\n",
    "\n",
    "# fake\n",
    "df['type'] = df['type'].replace(['conspiracy', 'fake'], 'fake')\n",
    "\n",
    "\n",
    "#reliable\n",
    "df['type'] = df['type'].replace(['unreliable', 'bias', 'unreliable', 'unknown'], 'reliable')\n",
    "print(df['type'].value_counts())\n",
    "\n",
    "# C: Examine the percentage distribution of 'reliable' vs. 'fake' articles. Is the dataset balanced? Discuss the importance of a balanced distribution.\n",
    "\n",
    "# reliable vs fake - percentage\n",
    "print(df['type'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Gathering Links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' from selenium import webdriver\\nfrom selenium.webdriver.common.by import By\\nfrom selenium.webdriver.common.action_chains import ActionChains\\nfrom selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException\\nfrom selenium.webdriver.support.ui import WebDriverWait\\nfrom selenium.webdriver.support import expected_conditions as EC\\nimport csv\\nimport time\\n\\n# ----------------------------------------------------------------\\n# 1) Initialize Selenium WebDriver (Firefox)\\n# ----------------------------------------------------------------\\nbrowser = webdriver.Firefox()\\nwait = WebDriverWait(browser, 10)\\n\\n# Track visited links to avoid duplicates\\nextracted_links = set()\\n\\n# ----------------------------------------------------------------\\n# 2) Reject cookies only once\\n# ----------------------------------------------------------------\\ndef reject_cookies_once(driver):\\n    try:\\n        iframe = wait.until(\\n            EC.frame_to_be_available_and_switch_to_it(\\n                (By.CSS_SELECTOR, \"iframe[id^=\\'sp_message_iframe\\']\")\\n            )\\n        )\\n        reject_button = wait.until(\\n            EC.element_to_be_clickable(\\n                (By.XPATH, \"//button[contains(text(), \\'I do not agree\\')]\")\\n            )\\n        )\\n        reject_button.click()\\n        print(\"Cookies rejected.\")\\n    except TimeoutException:\\n        print(\"No cookie popup detected (maybe already dismissed).\")\\n    except Exception as e:\\n        print(f\"Error handling cookies: {e}\")\\n    finally:\\n        driver.switch_to.default_content()\\n        time.sleep(2)\\n\\n# ----------------------------------------------------------------\\n# 3) Extract articles from the current page\\n# ----------------------------------------------------------------\\ndef extract_articles(driver):\\n    articles = []\\n    \\n    # Wait for at least one article\\n    try:\\n        wait.until(\\n            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article a[data-testid=\\'internal-link\\']\"))\\n        )\\n    except TimeoutException:\\n        print(\"No articles found on this page.\")\\n        return articles\\n    \\n    links = driver.find_elements(By.CSS_SELECTOR, \"article a[data-testid=\\'internal-link\\']\")\\n    \\n    for link_el in links:\\n        try:\\n            link = link_el.get_attribute(\"href\")\\n            title = link_el.text.strip() or \"No title\"\\n            summary = \"No summary\"  # Adjust if summaries are available\\n            \\n            if link not in extracted_links:\\n                extracted_links.add(link)\\n                articles.append({\\n                    \"title\": title,\\n                    \"summary\": summary,\\n                    \"link\": link\\n                })\\n        except Exception as e:\\n            print(f\"Error extracting an article: {e}\")\\n            continue\\n    \\n    print(f\"Extracted {len(articles)} new articles from this page.\")\\n    return articles\\n\\n# ----------------------------------------------------------------\\n# 4) Scrape all pages by clicking the “Next Page” button\\n# ----------------------------------------------------------------\\ndef scrape_region(driver, url):\\n    all_articles = []\\n    driver.get(url)\\n    time.sleep(3)\\n    \\n    page_number = 1\\n    max_pages = 50  # Limit max pages per region to prevent infinite loops\\n    \\n    while page_number <= max_pages:\\n        page_articles = extract_articles(driver)\\n        all_articles.extend(page_articles)\\n        \\n        try:\\n            next_button = driver.find_element(By.CSS_SELECTOR, \"button[data-testid=\\'pagination-next-button\\']\")\\n            \\n            if next_button.get_attribute(\"disabled\"):\\n                print(f\"No more pages (button disabled) at page {page_number}.\")\\n                break\\n\\n            driver.execute_script(\"arguments[0].scrollIntoView({block: \\'center\\'});\", next_button)\\n            time.sleep(1)\\n\\n            try:\\n                next_button.click()\\n            except ElementClickInterceptedException:\\n                ActionChains(driver).move_to_element(next_button).click().perform()\\n\\n            page_number += 1\\n            print(f\"Clicked \\'Next Page\\' -> now on page {page_number}...\")\\n            time.sleep(3)\\n        \\n        except NoSuchElementException:\\n            print(f\"No \\'Next Page\\' button found after page {page_number}.\")\\n            break\\n        except ElementNotInteractableException:\\n            print(f\"\\'Next Page\\' button present but not interactable on page {page_number}.\")\\n            break\\n    \\n    return all_articles\\n\\n# ----------------------------------------------------------------\\n# MAIN SCRIPT\\n# ----------------------------------------------------------------\\ntry:\\n    browser.get(\"https://www.bbc.com/news/world/europe\")\\n    time.sleep(3)\\n    reject_cookies_once(browser)\\n\\n    regions = [\\n        \"world/europe\"\\n    ]\\n    #, \"world/us_and_canada\", \"uk\", \"world/australia\", \"world/asia\",\\n    #    \"world/africa\", \"world/latin_america\", \"world/middle_east\"\\n    \\n    all_articles = []\\n    \\n    for region in regions:\\n        print(f\"\\n--- Scraping region: {region} ---\")\\n        url = f\"https://www.bbc.com/news/{region.lstrip(\\'/\\')}\"\\n        region_articles = scrape_region(browser, url)\\n        all_articles.extend(region_articles)\\n        time.sleep(2)\\n        \\nfinally:\\n    browser.quit()\\n\\n# Save to CSV\\nwith open(\"bbc_articles.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\\n    writer = csv.DictWriter(f, fieldnames=[\"title\", \"summary\", \"link\"])\\n    writer.writeheader()\\n    writer.writerows(all_articles)\\n\\nprint(f\"\\nDone! Extracted {len(all_articles)} unique articles in total.\")\\n '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Library Installation:\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# 2) Retrieve HTML Content:\n",
    "browser = webdriver.Firefox()\n",
    "wait = WebDriverWait(browser, 10)\n",
    "# Track visited links to avoid duplicates\n",
    "extracted_links = set()\n",
    "\n",
    "# Reject cookies only once (as an iframe)\n",
    "def reject_cookies(driver):\n",
    "    try:\n",
    "        iframe = wait.until(\n",
    "            EC.frame_to_be_available_and_switch_to_it(\n",
    "                (By.CSS_SELECTOR, \"iframe[id^='sp_message_iframe']\")\n",
    "            )\n",
    "        )\n",
    "        reject_button = wait.until(\n",
    "            EC.element_to_be_clickable(\n",
    "                (By.XPATH, \"//button[contains(text(), 'I do not agree')]\")\n",
    "            )\n",
    "        )\n",
    "        reject_button.click()\n",
    "        print(\"Cookies rejected\")\n",
    "    except TimeoutException:\n",
    "        print(\"No cookie popup detected\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error handling cookies: {e}\")\n",
    "    finally:\n",
    "        driver.switch_to.default_content()\n",
    "        time.sleep(2)\n",
    "\n",
    "# 3) Extract Articles:\n",
    "def extract_articles(driver):\n",
    "    articles = []\n",
    "    # Wait for at least one article\n",
    "    try:\n",
    "        wait.until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article a[data-testid='internal-link']\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(\"No articles found on this page\")\n",
    "        return articles\n",
    "    links = driver.find_elements(By.CSS_SELECTOR, \"article a[data-testid='internal-link']\")\n",
    "    for link_ele in links:\n",
    "        try:\n",
    "            title = link_ele.text.strip() or \"No title\"\n",
    "            summary = \"No summary\"\n",
    "            link = link_ele.get_attribute(\"href\")\n",
    "            if link not in extracted_links:\n",
    "                extracted_links.add(link)\n",
    "                articles.append({\n",
    "                    \"title\": title,\n",
    "                    \"summary\": summary,\n",
    "                    \"link\": link\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting an article: {e}\")\n",
    "            continue\n",
    "    print(f\"Extracted {len(articles)} new articles from this page\")\n",
    "    return articles\n",
    "\n",
    "# 4) Scrape Multiple Pages:\n",
    "def scrape_region(driver, url):\n",
    "    all_articles = []\n",
    "    driver.get(url)\n",
    "    time.sleep(3) \n",
    "    page_num = 1\n",
    "    # Limit to avoid infinite loops\n",
    "    max_pages = 50 \n",
    "    \n",
    "    while page_num <= max_pages:\n",
    "        page_articles = extract_articles(driver)\n",
    "        all_articles.extend(page_articles)\n",
    "        try:\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, \"button[data-testid='pagination-next-button']\")\n",
    "            if next_button.get_attribute(\"disabled\"):\n",
    "                print(f\"No more pages {page_num}\")\n",
    "                break\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_button)\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                next_button.click()\n",
    "            except ElementClickInterceptedException:\n",
    "                ActionChains(driver).move_to_element(next_button).click().perform()\n",
    "\n",
    "            page_num += 1\n",
    "            print(f\"Clicked 'Next Page' - page {page_num}\")\n",
    "            time.sleep(3)\n",
    "        \n",
    "        except NoSuchElementException:\n",
    "            print(f\"No 'Next Page' button on page {page_num}\")\n",
    "            break\n",
    "        except ElementNotInteractableException:\n",
    "            print(f\"'Next Page' button not interactable on page {page_num}\")\n",
    "            break\n",
    "    return all_articles\n",
    "\n",
    "# 5) Expand the Scope:\n",
    "try:\n",
    "    browser.get(\"https://www.bbc.com/news/world/europe\")\n",
    "    time.sleep(3)\n",
    "    reject_cookies(browser)\n",
    "\n",
    "    regions = [\n",
    "        \"world/europe\", \"world/us_and_canada\", \"uk\", \"world/australia\", \"world/asia\",\n",
    "        \"world/africa\", \"world/latin_america\", \"world/middle_east\"\n",
    "    ]\n",
    "    all_articles = []\n",
    "    \n",
    "    for region in regions:\n",
    "        print(f\"\\n Scraping region: {region}\")\n",
    "        url = f\"https://www.bbc.com/news/{region.lstrip('/')}\"\n",
    "        region_articles = scrape_region(browser, url)\n",
    "        all_articles.extend(region_articles)\n",
    "        time.sleep(2)\n",
    "finally:\n",
    "    browser.quit()\n",
    "\n",
    "# 6) Save Your Results:\n",
    "with open(\"bbc_articles.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"summary\", \"link\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_articles)\n",
    "\n",
    "print(f\"\\nDone! TOTAL unique Extracted articles: {len(all_articles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Scraping Article Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1) Article Inspection: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping: https://www.bbc.com/news/articles/c871e41751yo\n",
      "Scraping: https://www.bbc.com/news/articles/c3w14gw3wwlo\n",
      "Scraping: https://www.bbc.com/news/articles/cz7v1ejv01go\n",
      "Scraping: https://www.bbc.com/news/articles/ckg8jllq283o\n",
      "Scraping: https://www.bbc.com/news/articles/c0mwllzzyxlo\n",
      "Scraping: https://www.bbc.com/news/articles/cgq9n48el43o\n",
      "Scraping: https://www.bbc.com/news/articles/c4gmej22p8ro\n",
      "Scraping: https://www.bbc.com/news/articles/cq5z8deg27xo\n",
      "Scraping: https://www.bbc.com/news/articles/c981w25y5wpo\n",
      "Scraping: https://www.bbc.com/news/articles/c3d810xyvz9o\n",
      "Scraping: https://www.bbc.com/news/articles/czxnekw9lyjo\n",
      "Scraping: https://www.bbc.com/news/articles/cdjyxljyjxno\n",
      "Scraping: https://www.bbc.com/news/articles/ce98xnldr0vo\n",
      "Scraping: https://www.bbc.com/news/articles/ce8yz5dk82wo\n",
      "Scraping: https://www.bbc.com/news/articles/cwygwrzjxqzo\n",
      "Scraping: https://www.bbc.com/news/articles/cq6yg204pvmo\n",
      "Scraping: https://www.bbc.com/news/videos/czedyer9522o\n",
      "Scraping: https://www.bbc.com/news/videos/c798q33r9gjo\n",
      "Scraping: https://www.bbc.com/news/videos/cy05qgqj4j0o\n",
      "Scraping: https://www.bbc.com/news/videos/cvgw91rvn8xo\n",
      "Scraping: https://www.bbc.com/news/videos/c5y2pdz5wwyo\n",
      "Scraping: https://www.bbc.com/news/articles/c0l1673rx2lo\n",
      "Scraping: https://www.bbc.com/news/articles/cvgpd573mxzo\n",
      "Scraping: https://www.bbc.com/news/articles/c2017v5ngxpo\n",
      "Scraping: https://www.bbc.com/news/articles/cqlyez7dezpo\n",
      "Scraping: https://www.bbc.com/news/articles/cedlvv4j1vjo\n",
      "Scraping: https://www.bbc.com/news/articles/crknzzk8g75o\n",
      "Scraping: https://www.bbc.com/news/articles/cm2ym0g2kzvo\n",
      "Scraping: https://www.bbc.com/news/articles/cvge8dnrge5o\n",
      "Scraping: https://www.bbc.com/news/articles/c4gde1zj5pyo\n",
      "Scraping: https://www.bbc.com/news/articles/cly28dj107qo\n",
      "Scraping: https://www.bbc.com/news/articles/cwygxlyzq8zo\n",
      "Scraping: https://www.bbc.com/news/articles/cge1qq4jdxdo\n",
      "Scraping: https://www.bbc.com/news/videos/cgm1wxxvv2zo\n",
      "Scraping: https://www.bbc.com/news/articles/cj3nrznln7zo\n",
      "Scraping: https://www.bbc.com/news/articles/cwyge72pv55o\n",
      "Scraping: https://www.bbc.com/news/articles/crrdqdky9gxo\n",
      "Scraping: https://www.bbc.com/news/articles/c743znzplgqo\n",
      "Scraping: https://www.bbc.com/news/world-europe-60506682\n",
      "Scraping: https://www.bbc.com/news/articles/cvge70y4q91o\n",
      "Scraping: https://www.bbc.com/news/articles/crmjk9mjnwmo\n",
      "Scraping: https://www.bbc.com/news/articles/cy0dkzrer18o\n",
      "Scraping: https://www.bbc.com/news/articles/ckg1r4g08kwo\n",
      "Scraping: https://www.bbc.com/news/articles/cm2nk82j2g7o\n",
      "Scraping: https://www.bbc.com/news/articles/ce349pj3e5do\n",
      "Scraping: https://www.bbc.com/news/articles/c9vygkzkkrvo\n",
      "Scraping: https://www.bbc.com/news/articles/c0rz178el8go\n",
      "Scraping: https://www.bbc.com/news/articles/c8j0ggm3mdwo\n",
      "Scraping: https://www.bbc.com/news/articles/ce98v8mnxm3o\n",
      "Scraping: https://www.bbc.com/news/articles/cn04551n6wpo\n",
      "Scraping: https://www.bbc.com/news/articles/c2kgqdkx9jxo\n",
      "Scraping: https://www.bbc.com/news/videos/c24199l47d2o\n",
      "Scraping: https://www.bbc.com/news/articles/ce34v17dw53o\n",
      "Scraping: https://www.bbc.com/news/articles/cd655917g6qo\n",
      "Scraping: https://www.bbc.com/news/articles/cy05yqz1kq2o\n",
      "Scraping: https://www.bbc.com/news/articles/cgr2g4n4wvdo\n",
      "Scraping: https://www.bbc.com/news/articles/c3vw9p5lw5xo\n",
      "Scraping: https://www.bbc.com/news/articles/czjenv29jgdo\n",
      "Scraping: https://www.bbc.com/news/world-europe-59667938\n",
      "Scraping: https://www.bbc.com/news/articles/cj6787zzg79o\n",
      "Scraping: https://www.bbc.com/news/articles/c99np092vk1o\n",
      "Scraping: https://www.bbc.com/news/articles/c39vjnpxy1wo\n",
      "Scraping: https://www.bbc.com/news/articles/cm2yyrl0dj5o\n",
      "Scraping: https://www.bbc.com/news/articles/cy7x0d210e2o\n",
      "Scraping: https://www.bbc.com/news/articles/ce982zpz1k3o\n",
      "Scraping: https://www.bbc.com/news/articles/cx2gg8le1kpo\n",
      "Scraping: https://www.bbc.com/news/articles/c8j0e7ye4m4o\n",
      "Scraping: https://www.bbc.com/news/articles/c3e44qev1dvo\n",
      "Scraping: https://www.bbc.com/news/articles/czxnnzz558eo\n",
      "Scraping: https://www.bbc.com/news/articles/cq5zzn81229o\n",
      "Scraping: https://www.bbc.com/news/articles/cge11q4xwr3o\n",
      "Scraping: https://www.bbc.com/news/articles/cly7yxm3py5o\n",
      "Scraping: https://www.bbc.com/news/articles/cpq222rqv4po\n",
      "Scraping: https://www.bbc.com/news/articles/cy833v895e1o\n",
      "Scraping: https://www.bbc.com/news/articles/c0kggxm6gevo\n",
      "Scraping: https://www.bbc.com/news/articles/clyjj9lv5qvo\n",
      "Scraping: https://www.bbc.com/news/articles/c8e774zkeg6o\n",
      "Scraping: https://www.bbc.com/news/articles/c0q1188p1n2o\n",
      "Scraping: https://www.bbc.com/news/articles/c23444ddzjyo\n",
      "Scraping: https://www.bbc.com/news/articles/c0l11gr35gwo\n",
      "Scraping: https://www.bbc.com/news/articles/c17qe11wy52o\n",
      "Scraping: https://www.bbc.com/news/articles/c9de53y43lvo\n",
      "Scraping: https://www.bbc.com/news/articles/cdrxy1zp8mxo\n",
      "Scraping: https://www.bbc.com/news/articles/cwygwzd4wggo\n",
      "Scraping: https://www.bbc.com/news/articles/c70eky7l6pxo\n",
      "Scraping: https://www.bbc.com/news/articles/cdel95e0j9ko\n",
      "Scraping: https://www.bbc.com/news/articles/cx2g51lpzvko\n",
      "Scraping: https://www.bbc.com/news/articles/c4g03yxdx3ro\n",
      "Scraping: https://www.bbc.com/news/world-europe-18023383\n",
      "Scraping: https://www.bbc.com/news/articles/ckgdy6750w1o\n",
      "Scraping: https://www.bbc.com/news/videos/cddy92025gdo\n",
      "Scraping: https://www.bbc.com/news/articles/cy9dl4drr8lo\n",
      "Scraping: https://www.bbc.com/news/articles/cvgpe71pey8o\n",
      "Scraping: https://www.bbc.com/news/articles/cj30xr1vy2lo\n",
      "Scraping: https://www.bbc.com/news/articles/cwygdl5xxklo\n",
      "Scraping: https://www.bbc.com/news/articles/cr72v17z5jeo\n",
      "Scraping: https://www.bbc.com/news/articles/cjry2pv1wr5o\n",
      "Scraping: https://www.bbc.com/news/articles/cr526605r3ro\n",
      "Scraping: https://www.bbc.com/news/articles/cx29wlje6dno\n",
      "Scraping: https://www.bbc.com/news/articles/cpwd2001d10o\n",
      "Scraping: https://www.bbc.com/news/articles/cpwd209jd1jo\n",
      "Scraping: https://www.bbc.com/news/articles/c5y97x90553o\n",
      "Scraping: https://www.bbc.com/news/articles/cdxnzkyw7n1o\n",
      "Scraping: https://www.bbc.com/news/articles/cj920yvn9ezo\n",
      "Scraping: https://www.bbc.com/news/articles/cvgenlw94n3o\n",
      "Scraping: https://www.bbc.com/news/articles/cpv4n0dg3v3o\n",
      "Scraping: https://www.bbc.com/news/articles/cq5g6q5wnz3o\n",
      "Scraping: https://www.bbc.com/news/videos/cp8vnk524e0o\n",
      "\n",
      "Done! Extracted 108 articles with full text.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "browser = webdriver.Firefox()\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "# 2) Text Scraping Function:\n",
    "def scrape_article(url):\n",
    "    try:\n",
    "        browser.get(url)\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        title = soup.find(\"h1\").text.strip() if soup.find(\"h1\") else \"No title\"\n",
    "        author = \"No author\"\n",
    "        author_tag = soup.find(\"span\", class_=lambda x: x and \"sc-b42e7a8f\" in x)  \n",
    "        if author_tag:\n",
    "            author = author_tag.text.strip()\n",
    "        date = \"No date\"\n",
    "        date_tag = soup.find(\"time\")\n",
    "        if date_tag:\n",
    "            date = date_tag.text.strip()\n",
    "        content = \"\"\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        for p in paragraphs:\n",
    "            content += p.text.strip() + \"\\n\"\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content.strip()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load article from bbc_articles.csv\n",
    "article_links = []\n",
    "with open(\"bbc_articles.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        article_links.append(row[\"link\"])\n",
    "\n",
    "# 3) Scrape All Articles:\n",
    "scraped_articles = []\n",
    "for link in article_links:\n",
    "    print(f\"Scraping: {link}\")\n",
    "    article_data = scrape_article(link)\n",
    "    if article_data:\n",
    "        scraped_articles.append(article_data)\n",
    "    # Avoid getting blocked\n",
    "    time.sleep(2)  \n",
    "\n",
    "# 4) Data Storage:\n",
    "with open(\"bbc_scraped_articles.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"author\", \"date\", \"content\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(scraped_articles)\n",
    "print(f\"\\nDone! Extracted {len(scraped_articles)} articles\")\n",
    "# Close the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Discussion: Discuss whether it would make sense to include this newly acquired data in the dataset. Argue why or why not and if possible include statistics to support your claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Preservation\n",
    "Keep the data that you have scraped so you can use it for your Group Project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
