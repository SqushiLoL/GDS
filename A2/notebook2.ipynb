{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Library Installation:\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementNotInteractableException, ElementClickInterceptedException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# 2) Retrieve HTML Content:\n",
    "browser = webdriver.Firefox()\n",
    "wait = WebDriverWait(browser, 10)\n",
    "# Track visited links to avoid duplicates\n",
    "extracted_links = set()\n",
    "\n",
    "# Reject cookies only once (as an iframe)\n",
    "def reject_cookies(driver):\n",
    "    try:\n",
    "        iframe = wait.until(\n",
    "            EC.frame_to_be_available_and_switch_to_it(\n",
    "                (By.CSS_SELECTOR, \"iframe[id^='sp_message_iframe']\")\n",
    "            )\n",
    "        )\n",
    "        reject_button = wait.until(\n",
    "            EC.element_to_be_clickable(\n",
    "                (By.XPATH, \"//button[contains(text(), 'I do not agree')]\")\n",
    "            )\n",
    "        )\n",
    "        reject_button.click()\n",
    "        print(\"Cookies rejected\")\n",
    "    except TimeoutException:\n",
    "        print(\"No cookie popup detected\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error handling cookies: {e}\")\n",
    "    finally:\n",
    "        driver.switch_to.default_content()\n",
    "        time.sleep(2)\n",
    "\n",
    "# 3) Extract Articles:\n",
    "def extract_articles(driver):\n",
    "    articles = []\n",
    "    # Wait for at least one article\n",
    "    try:\n",
    "        wait.until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"article a[data-testid='internal-link']\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(\"No articles found on this page\")\n",
    "        return articles\n",
    "    links = driver.find_elements(By.CSS_SELECTOR, \"article a[data-testid='internal-link']\")\n",
    "    for link_ele in links:\n",
    "        try:\n",
    "            title = link_ele.text.strip() or \"No title\"\n",
    "            summary = \"No summary\"\n",
    "            link = link_ele.get_attribute(\"href\")\n",
    "            if link not in extracted_links:\n",
    "                extracted_links.add(link)\n",
    "                articles.append({\n",
    "                    \"title\": title,\n",
    "                    \"summary\": summary,\n",
    "                    \"link\": link\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting an article: {e}\")\n",
    "            continue\n",
    "    print(f\"Extracted {len(articles)} new articles from this page\")\n",
    "    return articles\n",
    "\n",
    "# 4) Scrape Multiple Pages:\n",
    "def scrape_region(driver, url):\n",
    "    all_articles = []\n",
    "    driver.get(url)\n",
    "    time.sleep(3) \n",
    "    page_num = 1\n",
    "    # Limit to avoid infinite loops\n",
    "    max_pages = 50 \n",
    "    \n",
    "    while page_num <= max_pages:\n",
    "        page_articles = extract_articles(driver)\n",
    "        all_articles.extend(page_articles)\n",
    "        try:\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, \"button[data-testid='pagination-next-button']\")\n",
    "            if next_button.get_attribute(\"disabled\"):\n",
    "                print(f\"No more pages {page_num}\")\n",
    "                break\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", next_button)\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                next_button.click()\n",
    "            except ElementClickInterceptedException:\n",
    "                ActionChains(driver).move_to_element(next_button).click().perform()\n",
    "\n",
    "            page_num += 1\n",
    "            print(f\"Clicked 'Next Page' - page {page_num}\")\n",
    "            time.sleep(3)\n",
    "        \n",
    "        except NoSuchElementException:\n",
    "            print(f\"No 'Next Page' button on page {page_num}\")\n",
    "            break\n",
    "        except ElementNotInteractableException:\n",
    "            print(f\"'Next Page' button not interactable on page {page_num}\")\n",
    "            break\n",
    "    return all_articles\n",
    "\n",
    "# 5) Expand the Scope:\n",
    "try:\n",
    "    browser.get(\"https://www.bbc.com/news/world/europe\")\n",
    "    time.sleep(3)\n",
    "    reject_cookies(browser)\n",
    "\n",
    "    regions = [\n",
    "        \"world/europe\", \"world/us_and_canada\", \"uk\", \"world/australia\", \"world/asia\",\n",
    "        \"world/africa\", \"world/latin_america\", \"world/middle_east\"\n",
    "    ]\n",
    "    all_articles = []\n",
    "    \n",
    "    for region in regions:\n",
    "        print(f\"\\n Scraping region: {region}\")\n",
    "        url = f\"https://www.bbc.com/news/{region.lstrip('/')}\"\n",
    "        region_articles = scrape_region(browser, url)\n",
    "        all_articles.extend(region_articles)\n",
    "        time.sleep(2)\n",
    "finally:\n",
    "    browser.quit()\n",
    "\n",
    "# 6) Save Your Results:\n",
    "with open(\"bbc_articles.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"summary\", \"link\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(all_articles)\n",
    "\n",
    "print(f\"\\nDone! TOTAL unique Extracted articles: {len(all_articles)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Scraping Article Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1) Article Inspection: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "browser = webdriver.Firefox()\n",
    "wait = WebDriverWait(browser, 10)\n",
    "\n",
    "# 2) Text Scraping Function:\n",
    "def scrape_article(url):\n",
    "    try:\n",
    "        browser.get(url)\n",
    "        time.sleep(3)\n",
    "        soup = BeautifulSoup(browser.page_source, \"html.parser\")\n",
    "        title = soup.find(\"h1\").text.strip() if soup.find(\"h1\") else \"No title\"\n",
    "        author = \"No author\"\n",
    "        author_tag = soup.find(\"span\", class_=lambda x: x and \"sc-b42e7a8f\" in x)  \n",
    "        if author_tag:\n",
    "            author = author_tag.text.strip()\n",
    "        date = \"No date\"\n",
    "        date_tag = soup.find(\"time\")\n",
    "        if date_tag:\n",
    "            date = date_tag.text.strip()\n",
    "        content = \"\"\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        for p in paragraphs:\n",
    "            content += p.text.strip() + \"\\n\"\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"author\": author,\n",
    "            \"date\": date,\n",
    "            \"content\": content.strip()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load article from bbc_articles.csv\n",
    "article_links = []\n",
    "with open(\"bbc_articles.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        article_links.append(row[\"link\"])\n",
    "\n",
    "# 3) Scrape All Articles:\n",
    "scraped_articles = []\n",
    "for link in article_links:\n",
    "    print(f\"Scraping: {link}\")\n",
    "    article_data = scrape_article(link)\n",
    "    if article_data:\n",
    "        scraped_articles.append(article_data)\n",
    "    # Avoid getting blocked\n",
    "    time.sleep(2)  \n",
    "\n",
    "# 4) Data Storage:\n",
    "with open(\"bbc_scraped_articles.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"title\", \"author\", \"date\", \"content\"])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(scraped_articles)\n",
    "print(f\"\\nDone! Extracted {len(scraped_articles)} articles\")\n",
    "# Close the browser\n",
    "browser.quit()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
